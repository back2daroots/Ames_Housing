# ğŸ¡ Ames Housing Prices â€” Machine Learning Regression Project

## ğŸ“Œ Overview
This project uses the **Ames Housing dataset** (a well-known Kaggle dataset) to predict house sale prices based on various property features.  
It demonstrates a full end-to-end machine learning workflow:

- Exploratory Data Analysis (EDA)
- Feature engineering
- Preprocessing pipelines (numeric + categorical features)
- Multiple regression models (linear & tree-based)
- Hyperparameter tuning with cross-validation
- Model comparison and experiment logging
- Error analysis and feature importance

---

## âš™ï¸ Project structure

housing-prices/
â”‚
â”œâ”€â”€ data/                  # raw and processed datasets (ignored in git)
â”œâ”€â”€ notebooks/             # Jupyter notebooks for experiments
â”œâ”€â”€ src/                   # source code (preprocessing, features, models)
â”œâ”€â”€ models/                # saved trained models (.joblib)
â”œâ”€â”€ plots/                 # visualizations (residuals, feature importance)
â”œâ”€â”€ experiments_log.csv    # experiment history
â”œâ”€â”€ requirements.in / .txt # dependencies
â”œâ”€â”€ environment.yml        # conda environment
â”œâ”€â”€ README.md              # project documentation
â””â”€â”€ .gitignore


---

## ğŸ“Š Models & Results
All models were trained using a unified pipeline with preprocessing and feature engineering.  
Hyperparameter tuning was performed via **GridSearchCV (5-fold CV)**, results logged in `experiments_log.csv`.

| Model        | CV RMSE | Test RMSE ($) | MAE ($) | RÂ²   |
|--------------|---------|---------------|---------|------|
| Ridge        | â€¦       | â€¦             | â€¦       | â€¦    |
| Lasso        | â€¦       | â€¦             | â€¦       | â€¦    |
| RandomForest | â€¦       | â€¦             | â€¦       | â€¦    |
| XGBoost      | â€¦       | â€¦             | â€¦       | â€¦    |
| LightGBM     | â€¦       | â€¦             | â€¦       | â€¦    |
| CatBoost     | â€¦       | â€¦             | â€¦       | â€¦    |

*(XGBoost currently achieves the best performance with RÂ² â‰ˆ 0.936.)*

---

## ğŸ“ˆ Visualizations
- Residuals vs Predicted values  
- Residuals histogram  
- Qâ€“Q plot of residuals  
- Built-in feature importance (tree-based models)  
- Permutation importance (model-agnostic)  

Example:  
![Residuals vs Predicted](plots/xgb_residuals_scatter.png)  
![Feature Importance](plots/xgb_feature_importance_builtin.png)

---
